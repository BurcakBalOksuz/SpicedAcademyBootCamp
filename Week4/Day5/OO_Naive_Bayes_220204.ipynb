{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General concept\n",
    "The naive bayes classifier uses the Bayes theorem and the concepts of _conditional probablities_:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A )}{P(B)}$$\n",
    "\n",
    "We want to find the class with the highest posterior probability. This is the class that we predict:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{k \\in {(1, 2)}}{\\arg \\max} ~ P(Y=k | X_1 = x_1, X_2 = x_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' rule applied to classification\n",
    "\n",
    "$$\n",
    "P(Y=k | X_1 = x_1, X_2 = x_2) = \\frac{P(X_1 = x_1, X_2 = x_2|Y=k)P(Y=k)}{P(X_1 = x_1, X_2 = x_2)}\n",
    "$$\n",
    "\n",
    "The denominator is a constant as it does not depend on the value of `k`. Dropping this term does not change the optimization problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \\underset{k \\in {(1, 2)}}{\\arg \\max} ~ P(Y=k | X_1 = x_1, X_2 = x_2) = \\underset{k \\in {(1, 2)}}{\\arg \\max} ~ P(X_1 = x_1, X_2 = x_2|Y=k)P(Y=k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make **\"naive\"** assumption that input **features are conditionally independent** from each other\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{k \\in {(1, 2)}}{\\arg \\max} ~ P(X_1 = x_1 | Y = k)P(X_2 = x_2|Y=k)P(Y=k)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our X is binomial distributed, we can now easily estimate $P(X_1 = x_1 | Y = 1), P(X_1 = x_1 | Y = 0), \\dots$ by looking at the data and counting relative frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the conditional probability that a given text belongs to an artist: $P (artist | text )$\n",
    "\n",
    "$$P(artist1|text) = \\frac{P(text|artist1)P(artist1)}{P(text)}$$\n",
    "\n",
    "and compare it to\n",
    "\n",
    "$$P(artist2|text) = \\frac{P(text|artist2)P(artist2 )}{P(text)}$$\n",
    "\n",
    "for each of the artists (class) and assigns the artist with the highest probability.\n",
    "\n",
    "$$\\text{IF:} P(artist1|text) > P(artist2|text): \\text{song artist1}$$\n",
    "\n",
    "$$\\text{IF:} P(artist2|text) > P(artist1|text): \\text{song artist2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical minimal example: \n",
    "- **text** = submarine  \n",
    "- **artist 1** = Beatles\n",
    "- **artist 2** = Eminem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate: $P(Beatles|submarine)$: \n",
    "\n",
    "$$P(Beatles|submarine) = \\frac{P(submarine|Beatles)P(Beatles )}{P(submarine)}$$\n",
    "\n",
    "- $P(submarine|Beatles)$ := probability of 'submarine' appearing in Beatles' songs\n",
    "- $P(Beatles)$ := probability of all Beatles' songs in the data set\n",
    "- $P(submarine)$: probability of 'submarine' appearing in any song in the data set.\n",
    "\n",
    "Compare to:\n",
    "\n",
    "$$P(Eminem|submarine) = \\frac{P(submarine|Eminem)P(Eminem )}{P(submarine)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more than 1 word, as we have many many words in our text corpus\n",
    "\n",
    "$$P( words | artist ) = P( word1 | artist ) * P( word2 | artist ) * .... * P(word_N | artist)$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$P(Eminem|(\\text{I have submarine})) = P(\\text{I have submarine})P(Eminem )$$\n",
    "\n",
    "$$P(Eminem|(\\text{I have submarine})) = P(\\text{I}|Eminem)*P(\\text{have}|Eminem)*P(\\text{submarine}|Eminem)*P(Eminem )$$\n",
    "\n",
    "And this where the word *naive* comes in the classifier, as it assumes all these probabilities are independent and it can just multiply them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems and solutions\n",
    "\n",
    "##### I. What about if we have a word that is not appearing and one of the termn  $P( word1 | artist )$ = 0?  \n",
    "---> Smoothing term `C`\n",
    "\n",
    "$P(Eminem|(\\text{I have submarine})) = P(\\text{I}|Eminem)*P(\\text{have}|Eminem)*P(\\text{submarine}|Eminem)*P(Eminem )$\n",
    "\n",
    "\n",
    "    * we assume that every word occurs k times at least\n",
    "    * so that probability is always > 0\n",
    "    * it's like assuming that the artist attached a copy of each word in the dictionary to the song\n",
    "    * IMPORTANT: If we increase the smoothing term, we *dilute* the information from the song, hende\n",
    "    * **the smoothing term is a regularization hyperparameter!**\n",
    "##### II. Don't we end with a very very small term if we multiply many very small probabilities? \n",
    "\n",
    "--> yes, this is problematic:\n",
    "\n",
    "$p(w1) \\cdot p(w2) ... \\cdot p(wn)$\n",
    "\n",
    "--> therefore: **Calculate log-probabilites instead:**\n",
    "\n",
    "$log(p(w1)) + log(p(w2)) + log(p(wn)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"we all love a yellow submarine\",             # Beatles\n",
    "    \"yesterday, my submarine was in love\",        # Beatles\n",
    "    \"we are love trouble with loyalty here\",      # Eminem\n",
    "    \"loyalty to us is worth more than love is\"    # Eminem\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = [\"Beatles\"]*2 + [\"Eminem\"]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preTrainedTextClassifier(text_features, text_labels):\n",
    "    \"\"\"Takes text features and labels.\n",
    "    Returns pretrained model on text data.\"\"\"\n",
    "    m = make_pipeline(\n",
    "        (TfidfVectorizer()),\n",
    "        (MultinomialNB())\n",
    "    )\n",
    "    m.fit(text_features, text_labels)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandPredictor(new_text_data, model):\n",
    "    \"\"\"Takes pretrained model and new text data.\n",
    "    Returns hard predictions on band.\"\"\"\n",
    "    new_text_data = [new_text_data]\n",
    "    band_predictions = model.predict(new_text_data)\n",
    "    probs = model.predict_proba(new_text_data)\n",
    "    return band_predictions[0], probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = preTrainedTextClassifier(corpus, text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_data = \"We love our loyal submarine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Beatles', array([[0.61988988, 0.38011012]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandPredictor(new_text_data, pre_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
